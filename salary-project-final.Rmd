---
title: "\\textbf{Modern Salary Modeling Project}"
author: "Justin Mai, Troy Russo, Isaac Muhlestein, Conan Li, Jian Kang"
output:
  pdf_document:
    toc: true
    latex_engine: xelatex
  html_document: default
geometry: margin=37pt
fontsize: 11pt
header-includes:
  - \usepackage{titling}
  - \usepackage{titlesec}
  - \titlespacing*{\title}{0pt}{0pt}{0pt}
  - \setlength{\droptitle}{-2em}
  - \setlength{\topskip}{0pt} 
mainfont: "Times New Roman"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r include=FALSE}
library(tidyverse)
library(leaps)
library(dplyr)
library(corrplot)
library(car)
library(MASS)
```

---

# 1. Introduction (Justin)

**Description:** The job market can be a hard place to navigate, especially with the search of data roles in the recent years. As statistics students, many of us are leaning towards opportunities within data roles. To understand the recent market we will be analyzing data job logistics to investigate the factors and predictors that most impact the salary of these roles. Within this report, we will be using Salary Index data reported by real people in the industry to (1) What predictors are the best for optimizing our model to predict salaries. We will discover the factors and variables within the job description that may influence a person's job salary the most to help students like us navigate the market (optimize using the AIC). (2) Based off our optimized model, how well does this predict the actual job salaries in the dataset? We will do this by splitting our data into training (80%) and testing (20%) data to test the final model.

**Disclaimer:** We have pivoted from using this dataset https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights which is comprised of synthetic data based off the current job market regarding AI jobs to this dataset https://www.kaggle.com/datasets/murilozangari/jobs-and-salaries-in-data-field-2024/data which consists of real survey data from various people in data roles, reporting through this website https://aijobs.net/salaries/2024/. We decided to make this change because we believe that variables such as `experience_level` and `job_category` which can be found in our current dataset would be strong predictors for `salary`. We also believe that using real survey data as supposed to synthetic data would give us results that are more related to real-life circumstances, making the report more applicable for all.

---

# 2. Methods

## 2.1 Data Description (Justin)

```{r echo=F}
job_data <- read.csv("jobs_in_data_2024.csv")

job_data <- job_data %>% 
  dplyr::select(!c(work_year, salary_currency, salary))

cost_of_living <- read.csv("cost_of_living_2024.csv")

cost_of_living <- cost_of_living %>% 
  rename(employee_residence = Country)
```

The first dataset was collected through https://aijobs.net/salaries/2024/, it consists of 14199 different observations, with each observation representing a person in their role in 2024. The **response variable** we are measuring is `salary_in_usd` which measures a person's annual gross salary. The **8 predictors** are `experience_level`, `employment_type`, `job_title`, `employee_residence`, `work_setting`, `company_location`, `company_size`, `job_category`. All of these variables are categorical where `company_size` is categorized as *S* for small, *M* for medium, and *L* for large.

The second dataset consists of cost of living index by country where an index of 100 represents the living cost of NYC, United States, with all indices relative to that value. We will merge the two datasets by `country`. The predictors we're looking at in this dataset are Cost of Living Index, Rent Index, Cost of Living Plus Rent Index, and Local Purchasing Power Index. We believe that the cost of living could be indicative of `salary_usd`.

## 2.2 Data Processing (Justin and Jian)

The primary dataset will be comprised of the two datasets described in (2.1). We are joining the two datasets on `employee_residence` which is in form of country. Now each row will consist of a specified job description along with the cost indexes for each respective resident. Having all of these predictors in one dataset will allow us to utilize the lm() function to uncover linear trends for all predictor variables in response to `salary`. It will also allow us to compare models easily which we will do using ANOVA tests and by calculating the F-statistic. The primary dataset consists of 14199 observations after joining

**Data Manipulation**: Rows that consisted of NAs were in countries that weren't listed in the `cost_of_living` data, this demonstrates that their rank is low when ordering by index and there weren't a sufficient number of samples for those countries. Therefore we removed those observations (14161 observations). We also removed exact duplicate rows from the dataset (7575 observations)

Mutations in the data were also made to create new predictors `us_resident` which is a binary variable that denotes if the job is in the U.S. or not, and `experience_numeric` which turns `experience_level` into numerical values (i.e. 1 - "Entry-Level", 2 - Mid-level", 3 - "Senior", 4 - "Executive"), this transformation will support our use of linear modeling and allow us to easily check assumptions such as linearity assumptions. Also, because we have too many different job titles, we decided to aggregate these job titles by keywords into 8 categories (Data Scientist, Data Analyst, Machine Learning, Data Engineer, Leadership, Business Intelligence, Research, Other). This will consolidate our data and make linear models more interpretable

We are also reordering values to ensure that our **baseline term** is what we want it to be (i.e. releveling small companies to be the first type and entry-level jobs to be the first job types). 

```{r label="data-manipulation", echo=FALSE}
# changing categorical to as.factor
job_data <- job_data %>%
  mutate(across(where(is.character), as.factor))

# Joining the two datasets
joined_df <- job_data %>% 
  left_join(cost_of_living, by=c("employee_residence"))

# removing NAs
joined_df <- na.omit(joined_df)

# new variable
joined_df <- joined_df %>% 
  mutate(us_resident = ifelse(employee_residence == "United States", 1, 0))

# changing experience level to a numeric
joined_df <- joined_df %>%
  mutate(experience_numeric = case_when(
    experience_level == "Entry-level" ~ 1,
    experience_level == "Mid-level" ~ 2,
    experience_level == "Senior" ~ 3,
    experience_level == "Executive" ~ 4
  ))

# ordering levels
joined_df$company_size <- factor(joined_df$company_size, levels = c("S", "M", "L"))
joined_df$experience_level <- factor(joined_df$experience_level, levels = c("Entry-level", "Mid-level", "Senior", "Executive"))

# aggregating job titles to job categories
joined_df <- joined_df %>%
  mutate(job_category = case_when(
    grepl("Data Scientist|Data Science|Integration|Applied Scientist", job_title, ignore.case = TRUE) ~ "Data Scientist",
    grepl("Analyst|Analytics|Modeler", job_title, ignore.case = TRUE) ~ "Data Analyst",
    grepl("Machine Learning|ML|AI", job_title, ignore.case = TRUE) ~ "Machine Learning",
    grepl("Engineer|Architect|Developer", job_title, ignore.case = TRUE) ~ "Data Engineer",
    grepl("Manager|Director|Lead|Head|Management", job_title, ignore.case = TRUE) ~ "Leadership",
    grepl("Business Intelligence|BI", job_title, ignore.case = TRUE) ~ "Business Intelligence",
    grepl("Research", job_title, ignore.case = TRUE) ~ "Research",
    TRUE ~ "Other"
  ))

# removing duplicate rows
joined_df <- joined_df %>% 
  distinct()
```

## 2.3 Model Diagnostics (Justin and Jian)

**Linear Modeling Assumptions**:

\begin{itemize}
    \item \textbf{Linearity}: The relationship between the predictor and response is linear.
    \item \textbf{Independence}: All observations are independent of one another (pair-wise independence).
    \item \textbf{Homoscedasticity}: The variance of residuals is constant across predictor levels.
    \item \textbf{Residual Normality}: The residuals follow a normal distribution.
\end{itemize}

We know that the reported job descriptions are all independent of one another through the data description.

The model that we are using seems highly categorical, even after our data transformation process, which would result in very discrete predictions. To fix this issue we want to create a new interaction term and turn it into a predictor variable, adding a continuous predictor for `salary_usd`. We hypothesize that salary growth will vary by location, therefore, we are creating an interaction term between `experience_numeric` and the Cost of Living Index to test this (experience combined with living costs could impact salary growth differently).

The residuals when using a non-transformed model is skewed due to the deviations within the tails in our qq-plot, to fix this issue, we would have to use a **log-transformation** on the data. The variance of the residuals is also constant and there is a clear linear and positive relationship between the predictor and response.

```{r message=FALSE, echo=FALSE, fig.width=5, fig.height=3, fig.align='center', fig.cap='Normality check for Experience and Cost of Living Index Interaction variable on Salary, comparison between no transformation and log-transformation', label="normality-check-1"}
# mutating data to create interaction term
joined_df <- joined_df %>% 
  mutate(exp_cost_int = experience_numeric * Cost.of.Living.Index)

par(mfrow = c(1, 2))
plot(lm(salary_in_usd ~ exp_cost_int, data = joined_df), which = 2)
title("No Transformation")
plot(lm(log(salary_in_usd) ~ log(exp_cost_int), data = joined_df), which = 2)
title("Log-Transformation")
par(mfrow = c(1, 1))  
```

To test the assumptions for our categorical variables we must look at the average salary by category, to see if there is a clear trend between the different experience levels in respective company size. This checks off the linearity assumption because there is a (somewhat) linear increasing trend for all the experience levels. We may explore an interaction term between `company_size` and `experience_level` because it seems like the trend for each `company_size` is increasing but at a different rate.

```{r message=FALSE, echo=FALSE, fig.width=4, fig.height=3, fig.align='center', fig.cap='Interaction between Experience Level and Company Size on Salary', label="interaction-check-1"}
mean_salary <- joined_df %>%
  group_by(experience_level, company_size) %>%
  summarize(mean_salary = mean(salary_in_usd))

ggplot(mean_salary, aes(x = experience_level, y = mean_salary, color = company_size, group = company_size)) +
  geom_point(size = 3) +
  geom_line() +
  labs(title = "Mean Salary by Exp. and Company Size") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


```{r include=FALSE}
library(dplyr)
model_df <- joined_df %>% 
  dplyr::select(!c(employment_type, work_setting, job_title, Cost.of.Living.Plus.Rent.Index, Local.Purchasing.Power.Index, Groceries.Index, Restaurant.Price.Index))
```
**Correlation and Multicollinearity Analysis (Isaac)**

One of the assumptions we are making in our model is that the predictors we are using are independent. Thus, it becomes expedient to test the multicollinearity of our predictors, to ensure that they are each independent of one another and add new information. Because a large majority of our predictors are categorical, we can use The VIF (Variance Inflation Factor) to test the multicollinearity of those:

```{r label="vif-check", eval=TRUE, echo=FALSE}
base_model <- lm(salary_in_usd~ experience_level + company_location+ 
                                company_size + job_category,
                 data = joined_df)

vif(base_model)
```

With the exception of `company_location`, all of these predictors have a VIF value that is lower than 5, so we can safely say that there are no issues of multicollinearity among them. Even `company_location` has a VIF of 6.527, so while it is above the baseline of 5 that we would like to see, it is not egregious. This implies that each of these predictors adds some new information to the model and thus explains (or doesn't) its own variation, which means that we need not fear that coefficients become over or underinflated, explaining variation that should be attributed to another predictor, or having that happen to them.

We are joining the cost of living dataset, which includes a number of variables that are related to cost of living metrics. We suspect that they might correlate together, being measures of the same changes in price and economy, so we can create a correlation matrix to examine any multicollinearity that may exist:

```{r label="correlation-vif", echo=FALSE, warning=FALSE, message=FALSE, fig.align= 'center'}

# 1. Correlation Matrix ---------------------------------------------
# Select numeric columns only (excluding the response itself if needed)
num_vars <- joined_df %>%
  select_if(is.numeric) #%>%
  # Optional: if you have columns like "salary_in_usd" or repeated ID columns, remove them:
  #select(-salary_in_usd)

# Compute correlation matrix (pairwise complete obs in case of any missing)
corr_mat <- cor(num_vars, use = "pairwise.complete.obs")

# Visualize the correlation matrix
corrplot(corr_mat, method = "circle", tl.cex = 0.7,
         title = "Correlation Matrix of Numeric Predictors",
         mar = c(0,0,1,0))

```

Examining our correlation matrix, we can see that the index variables are highly correlated with one another since the cost of living indices are joined by country, so there are several repeated values. To satisfy the multicollinearity assumption, we will be removing many of these highly correlated variables (only using `Cost.of.Living.Index`) for our MLR model.

```{r echo=FALSE}
model_df <- joined_df %>% 
  dplyr::select(!c(employment_type, job_title, Cost.of.Living.Plus.Rent.Index, Local.Purchasing.Power.Index, Groceries.Index, Restaurant.Price.Index, Rent.Index, employee_residence, company_location))
```

From our data processing and model diagnostics step, to avoid repeating variables, satisfy the assumptions necessary for conducting a linear model, and to consolidate the number of predictors we are using. Variables like `employement_type` and `job_title` are removed because they are highly correlated with new variables we mutated or existing variables. We are also removing `company_location` and `employee_residence` to simplify our model since there are a lot of different categories and most of them aren't significant to model at $\alpha = 0.05$, consolidating our model down. We'll be using a dataframe with the following variables: `r names(model_df)`.

```{r, echo = FALSE}
set.seed(123)  # Ensure reproducibility

# Calculate the total number of rows in model_df
n <- nrow(model_df)

# Randomly sample indices for the 20% test set
test_indices <- sample(seq_len(n), size = floor(0.2 * n))

# Create the test set and candidate (training) set
test_set <- model_df[test_indices, ]
candidate_set <- model_df[-test_indices, ]
```

## 2.4 Model Selection (Troy and Conan)

To determine our model, we will fit multiple linear regression models and find the model that minimizes our **AIC (Akaike Information Criterion)**. We favor the model selected by lowest AIC because BIC penalizes models with a large number of observations and tends to predict less than the AIC.

We can use a step function from the MASS package that computes the best model purely based on the AIC by comparing every potential model combination and returning the model with the lowest AIC. This model may be overfit however, since AIC does not account for model complexity.
```{r, echo = T}
# Fit the full model with all predictors
full_model <- lm(salary_in_usd ~ . - salary_in_usd, data = model_df)
# Perform stepwise selection using AIC (both directions)
step_model <- stepAIC(full_model, direction = "both", trace = FALSE)
```
Now that we have the model that we have found from AIC we can test the assumptions of the model as well as performing cross validation to test the valididity of the model. We can also perform an anova test to see if our predictors are significant.
```{r, echo = FALSE, fig.width=5, fig.height=3, fig.align= 'center'}
# Residuals vs Fitted
par(mfrow = c(1, 2))
plot(step_model$fitted.values, step_model$residuals,
     main = "Residuals vs Fitted",
     xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red")

# Q-Q Plot for Normality of Residuals
qqnorm(step_model$residuals, main = "Q-Q Plot")
qqline(step_model$residuals, col = "red")
par(mfrow = c(1, 2))
par(mfrow = c(1, 1))  
# Testing for heteroscedasticity using the Breusch-Pagan test
library(lmtest)
bp_test <- bptest(step_model)
 # p-value < 0.05 indicates potential heteroscedasticity

# 2. Model Validation: Cross-Validation -----------------------------

# Using the caret package for 10-fold cross-validation
library(caret)
library(knitr)
set.seed(123)  # for reproducibility

# Define training control for 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)

# Refit the model using caret's train() function. 
# Here, we use the same predictors that were selected in your step_model.
cv_model <- train(salary_in_usd ~ experience_level + job_title + employee_residence + 
                  work_setting + company_size, 
                  data = joined_df, 
                  method = "lm", 
                  trControl = train_control)
cv_results <- cv_model$results[, c("RMSE", "Rsquared", "MAE")]
kable(cv_results, caption = "10-fold Cross-Validation Results")


# 3. anova
anova_result <- anova(step_model)
kable(anova_result, caption = "anova test")
```
The cross validation tells us that our model is not over fitting since the $R^2$ is only about 35%, but also suggests that we can improve our model because the RMSE is quite high at 54993, so we may need to add more interactions or do some non linear transformations of the data. Additionally, the anova test suggests that our predictors are significant.

The residual plot shows a funnel shape, which suggests hetereoscadascity of the variance and the residuals do not appear to be centered at zero, which indicates that there is bias. The plots indicate that we may want to use the log of the salary instead of salary as our response.

We can now run the step AIC function again using the log(salary) as our response and perform the same model diagnostic tests as above. We also add an interaction term between experience_level and company size.

```{r, echo = FALSE, fig.width=5, fig.height=3, fig.align= 'center'}
model_df$log_salary <- log(model_df$salary_in_usd)
log_full_model <- lm(log_salary ~ .- salary_in_usd -log_salary, data = model_df)
step_log_model <- stepAIC(log_full_model, direction = "both", trace = FALSE)
model_with_interaction <- update(step_log_model, 
                                 . ~ . + experience_level:company_size)

# 2. Perform stepwise selection again starting from the updated model
step_log_model_interact <- stepAIC(model_with_interaction, 
                                  direction = "both", 
                                 trace = FALSE)

# 3. Review the summary of the new model
model_summary <- summary(step_log_model_interact)


# Print only the Call (Formula)
cat("Call:\n")
print(model_summary$call)
cat("\n")

cat("Multiple R-squared:", model_summary$r.squared, "\n")
par(mfrow = c(1, 2))
plot(step_log_model_interact$fitted.values, step_log_model_interact$residuals,
     main = "Residuals vs Fitted",
     xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red")

qqnorm(step_log_model_interact$residuals, main = "Q-Q Plot")
qqline(step_log_model_interact$residuals, col = "red")
par(mfrow = c(1, 1))  
bp_test <- bptest(step_log_model_interact)  # p-value < 0.05 indicates potential heteroscedasticity

anova_result <- anova(step_log_model_interact)
```


Our **log-transformed model** achieves a notably higher $R^2$, indicating it explains more variability in \(\log(\text{salary})\) than the untransformed model. However, the residual plot still exhibits heteroscedasticity, suggesting that further transformations or alternative modeling approaches may be necessary.

```{r, echo = FALSE, fig.width=4, fig.height=2.5, fig.align= 'center'}
bc <- boxcox(lm(salary_in_usd ~ -salary_in_usd, data = model_df), plotit = TRUE)
# Choose lambda based on the plot and refit using transformed response:
lambda_opt <- bc$x[which.max(bc$y)]
model_df$trans_salary <- if(lambda_opt == 0) log(model_df$salary_in_usd) else (model_df$salary_in_usd^lambda_opt - 1)/lambda_opt

```
The box cox indicates that the log transformed model is the best power model for our data, so the changes we need to make will be on individual predictors and not the entire data.

**Model Selection Variation (Isaac)**
One way to verify that this model is the optimal choice of predictors it to explore which predictors would be selected by the BIC and Cp criteria, and analyze any differences that exist.
```{r, echo=F, message=F, results='hide', fig.height=3,fig.width=4}
subsets <- regsubsets(data = model_df,
                      x = log(salary_in_usd)~.,
                      y = salary_in_usd,
                      nbest =1,
                      nvmax =16)
aic <- numeric(17)
for(i in 1:17){
  aic[i] = summary(subsets)$bic[i] + 2*i -i*log(7575) + 1000
}
cp_model <- which(summary(subsets)$cp == min(summary(subsets)$cp))
bic_model <- which(summary(subsets)$bic == min(summary(subsets)$bic))

df_model_selection <- data.frame(Criteria = c(rep("BIC",17),rep("Cp",17),rep("AIC+1000",17)),
                                 nv = c(1:17,1:17,1:17),
                                 val = c(summary(subsets)$bic,
                                         summary(subsets)$cp,
                                         aic))

ggplot(data = df_model_selection,
       mapping = aes(x = nv,
                     y = val,
                     color = Criteria))+
  geom_point()+
  theme_bw()+
  labs(title = "Criterion Values for AIC, BIC and Cp Optimization",
       x = "Number of Variables in Model",
       y = "Criterion Value")+
  geom_segment(x = 15,
               xend = 15,
               y = -4000,
               yend = -3000,
               color = "green",
               linetype = 2)+
  geom_segment(x = 16,
               xend = 16,
               y = 0,
               yend = 1000,
               color = "blue",
               linetype = 2)+
  geom_segment(x=16,
               xend = 16,
               y = -3000,
               yend = -2000,
               color = "red",
               linetype = 2)
```
The BIC criterion minimizes at 15 predictors, while the Cp and AIC criteria minimize at 16 predictors. We expect the BIC to minimize at a lower amount of predictors than the rest, but the fact that it is so close, alongside the fact that the Cp criteria minimizes at the same point as our AIC model, is a good indication that our model is optimal. 

**Cook's Distance Analysis (Isaac)**
We can also test to make sure that there are no high leverage points, that might be skewing the values of the coefficients. To do this, we can calculate the Cook's Distance on this model, and create a plot that visualizes these distances for each point. 

```{r, fig.label = "cooks-distance-and-leverage", echo = FALSE, fig.width=6, fig.height=3, fig.align= 'center'}

cooks_values <- cooks.distance(step_log_model)

threshhold <- 4/length(cooks_values)

par(mfrow=c(1,2))
barplot(cooks_values,
        main = "Cook's Distance",
        ylab = "Cook's Distance",
        xlab = "Observation Index")
abline(h = 4/7575,
       col = "red",
       lty = 2)
plot(step_log_model, which = 5)
```

Considering we have 7575 observations, a good threshold is that Cook's distance values should be lower than $\frac{4}{7575}\approx 0.00053$. Visibly, we can see that there are definitely some points that exceed this threshold, especially through the residual vs leverage plot there are some data points that apparently have high leverage. When plugging this into R, we get that 457 observations have significantly high values. This suggests that a non-trivial section of the data set is having a large impact on the regression model. We can take a quick look at the points associated with these highest values to make sure there is no mismeasurement going on.


```{r, echo = FALSE}
top_5_cook <- tail(sort(cooks_values),5)
#We get the numbers 
model_df[c(6845,7357,6052,6800,6030),c(2,5,6,7,8)]
```

Upon further analysis, it appears that the `Rank` predictor significantly increases the Cook's distance of high-leverage points. Removing this predictor from our model, we decrease the Cook's distances across the board, resulting in 84 fewer high-leverage points. While it is not ideal that we still have a significant number of high-leverage points, we are working with a very large data set, so it isn't too concerning.

## 2.5 Final Model Description (Troy, Conan, Isaac)

After comparing multiple candidate models, our final chosen model regresses **log(salary_in_usd)** on several predictors:
  
\begin{align*}
\log(\text{salary\_in\_usd}) &= 
\beta_0 
+ \beta_1 \cdot \text{experience\_level} \notag \\
&\quad + \beta_2 \cdot \text{job\_category} 
+ \beta_3 \cdot \text{us\_resident} \notag \\
&\quad + \beta_4 \cdot \text{work\_setting} 
+ \beta_5 \cdot \text{company\_size} \notag \\
&\quad + \beta_6 \cdot (\text{experience\_level} \times \text{company\_size}) \notag \\
&\quad + \beta_7 \cdot \text{exp\_cost\_int}
\end{align*}



### 1. **Log Transformation**

We transformed the salary to \(\log(\text{salary\_usd})\) to address heteroscedasticity and non‐normal residuals discovered in the initial model. Interpreting coefficients on the log‐scale means that each unit increase in a predictor corresponds to a *multiplicative* change in salary, rather than an additive one. For example:
- If \(\beta_1 = 0.10\) for a given predictor \(X\), then a **1‐unit** increase in \(X\) is associated with about a **10.5%** increase in salary \(\bigl(e^{0.10} - 1 \approx 0.105\bigr)\).

### 2. **Significant Predictors**

Based on the final model summary (not shown here in detail), we typically see the following patterns:

- **experience_level**: Highly significant, with more senior roles (e.g., “Senior,” “Executive”) associated with higher \(\log(\text{salary})\). This implies strong upward salary trends as experience grows.

- **company_size**: Medium or large companies might pay more, on average, than small companies—but the magnitude and significance can vary by the data.

- **job_category**: Certain roles (e.g., “Data Scientist,” “Machine Learning”) often command higher salaries relative to baseline roles.

- **us_resident**: Countries or regions with different cost‐of‐living indices can have substantially different salary norms (US vs non-US).

- **work_setting**: If “Remote” vs. “In‐Office” or other setups are included, each may show a different average salary level.

- **exp_cost_int**: People with varying experiences will likely have a different cost of living rate. 

### 3. **Interaction: `experience_level : company_size`**

We included an interaction between **experience_level** and **company_size** to test whether the *effect* of experience on salary depends on the size of the company. Interpreting an interaction on the log‐scale:

- The coefficient of `(Senior : Large)` tells us the additional log‐salary *beyond* simply adding the main effects of “Senior” and “Large” separately.  

# 3 Testing and Results (Troy)

```{r test-set-results, echo=FALSE}
# 1. Fit the final model on the "candidate_set"
final_model <- update(step_log_model_interact,.~.-Rank)

# 2. Generate predictions on the test set
test_predictions_log <- predict(final_model, newdata = test_set)

# 3. Evaluate performance
#    (a) MSE on the log scale
test_mse_log <- mean((log(test_set$salary_in_usd) - test_predictions_log)^2)

#    (b) RMSE on the log scale
test_rmse_log <- sqrt(test_mse_log)
test_rmse_log

#    (c) Convert predictions from log-salary to salary
test_predictions_salary <- exp(test_predictions_log)

#    (d) MSE on the *original* salary scale
test_mse_salary <- mean((test_set$salary_in_usd - test_predictions_salary)^2)
test_rmse_salary <- sqrt(test_mse_salary)
test_rmse_salary
```

\begin{table}[h]
    \centering
    \caption{Root Mean Squared Error (RMSE) Results}
    \label{tab:test_results}
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Metric} & \textbf{Value} \\
        \hline
        Log-Scale RMSE & 0.4008 \\
        \hline
        Original Salary RMSE & 59,804.53 \\
        \hline
    \end{tabular}
\end{table}


Our final model exhibits an RMSE of approximately 0.4 in log-scale, which translates to substantial error when converted back to the original salary scale. Specifically, the RMSE in USD is $59,761.05, which is about 0.4 times the median salary in our dataset. While this indicates that our model captures general salary trends, the remaining variability suggests inherent limitations in the available data, such as unobserved factors influencing salary that our model cannot account for.


```{r test-set-plot, echo=FALSE, fig.width=4, fig.height=3, fig.align= 'center'}
results_df <- data.frame(
  actual_salary = test_set$salary_in_usd,
  predicted_salary = test_predictions_salary
)

ggplot(results_df, aes(x = actual_salary, y = predicted_salary)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(title = "Test Set: Predicted vs. Actual Salary",
       x = "Actual Salary (USD)",
       y = "Predicted Salary (USD)") +
  theme_minimal()
```

The scatter plot of predicted salaries (y-axis) versus actual salaries (x-axis) shows that while our model captures the overall upward trend (points cluster around the diagonal), there is still considerable spread—particularly at higher salary levels. This indicates that although many predictions are reasonable, significant variability remains, suggesting unmodeled factors or inherent randomness in the data.
  
# 4. Discussion

## 4.1 Limitations (Justin)

The primary limitation to our study is that our data comes from self-reported surveys which could possibly hinder the accuracy of true data science and AI job salaries and resulted in many duplicates. Recognizing the many duplicates, we've removed exact duplicate observations, ensuring that each row is unique because its unlikely that two people have the exact same job description. We are also consolidating our data to US based workers vs. non-US based workers. This leads to issues because there are more US based observations in our dataset but there are still a significant number of reports outside of the US. Although almost all of the categorical predictors regarding `company_location` were considered insignificant to our model, there were still a few that were significant that we removed, which may cause slight inaccuracies to our prediction. Lastly, our Cost of Living and Job Salaries data are country-based, which isn't ideal for predicting salary because within each country, there are cities or provinces that pay higher salaries than others (NYC, San Francisco, and Seattle in the U.S.). This made our model one-dimensional since we aren't able to access which city each observation is from.

## 4.2 Conclusion (Conan)

In this study, we explored some key factors that influence salaries in data science related jobs using real world data. Through extensive data processing, model selection and log transformation, we were able to develop a model that improves the normality and heteroscedasticity in predicting salaries. The analysis focused on multiple predictors, as we found out **experience_level**, **company_size**, **job_category**, **us_resident**, **exp_cost_int** and **work_setting** are the most significant in predicting **salary**. We also added an interaction term between **experience_level** and **company_size** to see if the impact company size has on  salary varies significantly based on experience. The relatively high RMSE from our model suggests that other unobserved factors may also play a significant role in predicting salaries. Given the limitations, such as self-reported survey and locations, they suggest further research on additional predictors that may potentially have a significant impact on salaries. Overall, our model provides close insights into the salary trend in data science related job market that may help people searching for appropriate positions and making career decisions, and future exploration on location and cost of living may identify more complex relationships that will improve model accuracy. 
