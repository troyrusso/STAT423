---
title: "\\textbf{Modern Salary Modeling Project}"
author: "Justin Mai, Troy Russo, Isaac Muhlestein, Conan Li, Jian Kang"
output:
  pdf_document:
    toc: true
    latex_engine: xelatex
  html_document: default
geometry: margin=37pt
fontsize: 11pt
header-includes:
  - \usepackage{titling}
  - \usepackage{titlesec}
  - \titlespacing*{\title}{0pt}{0pt}{0pt}
  - \setlength{\droptitle}{-2em}
  - \setlength{\topskip}{0pt} 
mainfont: "Times New Roman"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
library(tidyverse)
library(leaps)
library(dplyr)
library(corrplot)
library(car)
library(MASS)
```

---

# 1. Introduction

**Description:** The job market can be a hard place to navigate, especially with the search of data roles in the recent years. As statistics students, many of us are leaning towards opportunities within data roles. To understand the recent market we will be analyzing data job logistics to investigate the factors and predictors that most impacts the salary of these roles. Within this report, we will be using Salary Index data reported by real people in the industry to (1) discover the factors and variables within the job description that may influence a person's job salary the most to help students like us navigate the market and (2) if there's a difference between the criterion (AIC and BIC), we will split our data into training and testing data to compare the two models optimized by the criterion, if not, we will still split the data to test the our optimized model.

**Disclaimer:** We have pivoted from using this dataset https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights which is comprised of synthetic data based off the current job market regarding AI jobs to this dataset https://www.kaggle.com/datasets/murilozangari/jobs-and-salaries-in-data-field-2024/data which consists of real survey data from various people in data roles, reporting through this website https://aijobs.net/salaries/2024/. We decided to make this change because we believe that variables such as `experience_level` and `job_category` which can be found in our current dataset would be strong predictors for `salary`. We also believe that using real survey data as supposed to synthetic data would give us results that are more related to real-life circumstances, making the report more applicable for all.

---

# 2. Methods

## 2.1 Data Description

```{r echo=F}
job_data <- read.csv("jobs_in_data_2024.csv")

job_data <- job_data %>% 
  dplyr::select(!c(work_year, salary_currency, salary))

cost_of_living <- read.csv("cost_of_living_2024.csv")

cost_of_living <- cost_of_living %>% 
  rename(employee_residence = Country)
```

The first dataset was collected through https://aijobs.net/salaries/2024/, it consists of 14199 different observations, with each observation representing a person in their role in 2024. The **response variable** we are measuring is `salary_in_usd` which measures a person's annual gross salary. The **8 predictors** are `experience_level`, `employment_type`, `job_title`, `employee_residence`, `work_setting`, `company_location`, `company_size`, `job_category`. All of these variables are categorical where `company_size` is categorized as *S* for small, *M* for medium, and *L* for large.

The second dataset consists of cost of living index by country where an index of 100 represents the living cost of NYC, United States, so all the indices are relative to that. We will merge the two datasets by `country`. The predictors we're looking at in this dataset are Cost of Living Index, Rent Index, Cost of Living Plus Rent Index, and Local Purchasing Power Index. We believe that the cost of living could be indicative of `salary_usd`.

## 2.2 Data Processing

The primary dataset will be comprised of the two datasets described in (2.1). We are joining the two datasets on `employee_residence` which is in form of country. Now each row will consist of a specified job description along with the cost indexes for each respective resident. Having all of these predictors in one dataset will allow us to utilize the lm() function to uncover linear trends for all predictor variables in response to `salary`. It will also allow us to compare models easily which we will do using ANOVA tests and by calculating the F-statistic. The primary dataset consists of 14199 observations after joining

**Data Manipulation**: Rows that consisted of NAs were in countries that weren't listed in the `cost_of_living` data, this demonstrates that their rank is low when ordering by index and there weren't a sufficient number of samples for those countries. Therefore we removed those observations (14161 observations). We also removed exact duplicate rows from the dataset (7575 observations)

Mutations in the data were also made to create new predictors `us_resident` which is a binary variable that denotes if the job is in the U.S. or not, and `experience_numeric` which turns `experience_level` into numerical values (i.e. 1 - "Entry-Level", 2 - Mid-level", 3 - "Senior", 4 - "Executive"), this transformation will support our use of linear modeling and allow us to easily check assumptions such as linearity assumptions. Also, because we have too many different job titles, we decided to aggregate these job titles by keywords into 8 categories (Data Scientist, Data Analyst, Machine Learning, Data Engineer, Leadership, Business Intelligence, Research, Other). This will consolidate our data and make linear models more interpretable

We are also reordering values to ensure that our **baseline term** is what we want it to be (i.e. releveling small companies to be the first type and entry-level jobs to be the first job types). 

```{r label="data-manipulation", echo=FALSE}
# changing categorical to as.factor
job_data <- job_data %>%
  mutate(across(where(is.character), as.factor))

# Joining the two datasets
joined_df <- job_data %>% 
  left_join(cost_of_living, by=c("employee_residence"))

# removing NAs
joined_df <- na.omit(joined_df)

# new variable
joined_df <- joined_df %>% 
  mutate(us_resident = ifelse(employee_residence == "United States", 1, 0))

# changing experience level to a numeric
joined_df <- joined_df %>%
  mutate(experience_numeric = case_when(
    experience_level == "Entry-level" ~ 1,
    experience_level == "Mid-level" ~ 2,
    experience_level == "Senior" ~ 3,
    experience_level == "Executive" ~ 4
  ))

# ordering levels
joined_df$company_size <- factor(joined_df$company_size, levels = c("S", "M", "L"))
joined_df$experience_level <- factor(joined_df$experience_level, levels = c("Entry-level", "Mid-level", "Senior", "Executive"))

# aggregating job titles to job categories
joined_df <- joined_df %>%
  mutate(job_category = case_when(
    grepl("Data Scientist|Data Science|Integration|Applied Scientist", job_title, ignore.case = TRUE) ~ "Data Scientist",
    grepl("Analyst|Analytics|Modeler", job_title, ignore.case = TRUE) ~ "Data Analyst",
    grepl("Machine Learning|ML|AI", job_title, ignore.case = TRUE) ~ "Machine Learning",
    grepl("Engineer|Architect|Developer", job_title, ignore.case = TRUE) ~ "Data Engineer",
    grepl("Manager|Director|Lead|Head|Management", job_title, ignore.case = TRUE) ~ "Leadership",
    grepl("Business Intelligence|BI", job_title, ignore.case = TRUE) ~ "Business Intelligence",
    grepl("Research", job_title, ignore.case = TRUE) ~ "Research",
    TRUE ~ "Other"
  ))

# removing duplicate rows
joined_df <- joined_df %>% 
  distinct()
```

## 2.3 Model Diagnostics

**Linear Modeling Assumptions**:

\begin{itemize}
    \item \textbf{Linearity}: The relationship between the predictor and response is linear.
    \item \textbf{Independence}: All observations are independent of one another (pair-wise independence).
    \item \textbf{Homoscedasticity}: The variance of residuals is constant across predictor levels.
    \item \textbf{Residual Normality}: The residuals follow a normal distribution.
\end{itemize}

We know that the reported job descriptions are all independent of one another through the data description.

The model that we are using seems highly categorical, even after our data transformation process, which would result in very discrete predictions. To fix this issue we want to create a new interaction term and turn it into a predictor variable, adding a continuous predictor for `salary_usd`. We hypothesize that salary growth will vary by location, therefore, we are creating an interaction term between `experience_numeric` and the Cost of Living Index to test this (experience combined with living costs could impact salary growth differently).

The residuals when using a non-transformed model is skewed due to the deviations within the tails in our qq-plot, to fix this issue, we would have to use a **log-transformation** on the data. The variance of the residuals is also constant and there is a clear linear and positive relationship between the predictor and response.

```{r message=FALSE, echo=FALSE, fig.width=5, fig.height=3, fig.align='center', fig.cap='Normality check for Experience and Cost of Living Index Interaction variable on Salary, comparison between no transformation and log-transformation', label="normality-check-1"}
# mutating data to create interaction term
joined_df <- joined_df %>% 
  mutate(exp_cost_int = experience_numeric * Cost.of.Living.Index)

par(mfrow = c(1, 2))
plot(lm(salary_in_usd ~ exp_cost_int, data = joined_df), which = 2)
title("No Transformation")
plot(lm(log(salary_in_usd) ~ log(exp_cost_int), data = joined_df), which = 2)
title("Log-Transformation")
par(mfrow = c(1, 1))  
```

To test the assumptions for our categorical variables we must look at the average salary by category, to see if there is a clear trend between the different experience levels in respective company size. This checks off the linearity assumption because there is a (somewhat) linear increasing trend for all the experience levels. We can see that we don't need an interaction effect between `experience_level` and `company_size` because the trend is increasing for each company size at around the same rate. We can test this using an ANOVA test at $\alpha = 0.05$ to see if adding an interaction term effects the model.

```{r message=FALSE, echo=FALSE, fig.width=5, fig.height=3, fig.align='center', fig.cap='Interaction between Experience Level and Company Size on Salary', label="interaction-check-1"}
mean_salary <- joined_df %>%
  group_by(experience_level, company_size) %>%
  summarize(mean_salary = mean(salary_in_usd))

ggplot(mean_salary, aes(x = experience_level, y = mean_salary, color = company_size, group = company_size)) +
  geom_point(size = 3) +
  geom_line() +
  labs(title = "Mean Salary by Experience Level and Company Size") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

**Correlation and Multicollinearity Analysis**

One of the assumptions we are making in our model is that the predictors we are using are independent. Thus, it becomes expedient to test the multicollinearity of our predictors, to ensure that they are each independent of one another and add new information. Because a large majority of our predictors are categorical, The VIF (Variance Inflation Factor) will be most effective in calculating that multicollinearity:

```{r label="vif-check", eval=TRUE, echo=FALSE}
base_model <- lm(salary_in_usd~ experience_level + company_location+ 
                                company_size + job_category + Rank,
                 data = joined_df)

vif(base_model)
```

```{r label="correlation-vif", echo=FALSE, warning=FALSE, message=FALSE}

# 1. Correlation Matrix ---------------------------------------------
# Select numeric columns only (excluding the response itself if needed)
num_vars <- joined_df %>%
  select_if(is.numeric) #%>%
  # Optional: if you have columns like "salary_in_usd" or repeated ID columns, remove them:
  #select(-salary_in_usd)

# Compute correlation matrix (pairwise complete obs in case of any missing)
corr_mat <- cor(num_vars, use = "pairwise.complete.obs")

# Visualize the correlation matrix
corrplot(corr_mat, method = "circle", tl.cex = 0.7,
         title = "Correlation Matrix of Numeric Predictors",
         mar = c(0,0,1,0))

```

Examining our correlation matrix, we can see that the index variables are highly correlated with one another since the cost of living indices are joined by country, so there are several repeated values. To satisfy the multicollinearity assumption, we will be removing many of these highly correlated variables (only using `Cost.of.Living.Index`) for our MLR model. 

```{r echo=FALSE}
model_df <- joined_df %>% 
  dplyr::select(!c(employment_type, job_title, Cost.of.Living.Plus.Rent.Index, Local.Purchasing.Power.Index, Groceries.Index, Restaurant.Price.Index, Rent.Index))
```

From our data processing and model diagnostics step, to avoid repeating variables, satisfy the assumptions necessary for conducting a linear model, and to consolidate the number of predictors we are using, we'll be using a dataframe with the following variables: `r names(model_df)`.

```{r}
set.seed(123)  # Ensure reproducibility

# Calculate the total number of rows in model_df
n <- nrow(model_df)

# Randomly sample indices for the 20% test set
test_indices <- sample(seq_len(n), size = floor(0.2 * n))

# Create the test set and candidate (training) set
test_set <- model_df[test_indices, ]
candidate_set <- model_df[-test_indices, ]
```

## 2.4 Model Selection

To determine our model, we will fit multiple linear regression models and find the model that minimizes our **AIC (Akaike Information Criterion) and/or BIC (Bayesian Information Criterion)**. If the AIC and BIC suggest different models, we will favor the model selected by lowest AIC because BIC penalizes models with a large number of observations and tends to predict less than the AIC.

We can use a step function from the MASS package that computes the best model purely based on the AIC by comparing every potential model combination and returning the model with the lowest AIC. This model may be overfit however, since AIC does not account for model complexity.
```{r, echo = F}

# Fit the full model with all predictors
full_model <- lm(salary_in_usd ~ . - salary_in_usd, data = model_df)

# Perform stepwise selection using AIC (both directions)
step_model <- stepAIC(full_model, direction = "both", trace = FALSE)

# Display the summary of the selected model
summary(step_model)
```
Now that we have the model that we have found from AIC we can test the assumptions of the model as well as performing cross validation to test the valididity of the model. We can also perform an anova test to see if our predictors are significant.
```{r}
# Assuming your final model is stored in 'step_model' and your data frame is 'joined_df'

# Alternatively, you can plot individual diagnostic plots:
# Residuals vs Fitted
plot(step_model$fitted.values, step_model$residuals,
     main = "Residuals vs Fitted",
     xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red")

# Q-Q Plot for Normality of Residuals
qqnorm(step_model$residuals, main = "Q-Q Plot")
qqline(step_model$residuals, col = "red")

# Testing for heteroscedasticity using the Breusch-Pagan test
library(lmtest)
bp_test <- bptest(step_model)
print(bp_test)  # p-value < 0.05 indicates potential heteroscedasticity

# 2. Model Validation: Cross-Validation -----------------------------

# Using the caret package for 10-fold cross-validation
# Install caret if needed: install.packages("caret")
library(caret)
set.seed(123)  # for reproducibility

# Define training control for 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)

# Refit the model using caret's train() function. 
# Here, we use the same predictors that were selected in your step_model.
cv_model <- train(salary_in_usd ~ experience_level + job_title + employee_residence + 
                  work_setting + company_size, 
                  data = joined_df, 
                  method = "lm", 
                  trControl = train_control)
print(cv_model)


# 3. Theoretical Considerations -------------------------------------
# Although not strictly "code," here are some steps to integrate theory:

# - Review each predictor’s significance and consider if domain knowledge suggests its retention.
#   For example, even if certain levels of a categorical predictor have high p-values, the variable might still be
#   important overall. You could run an ANOVA to test the overall significance of categorical variables:
anova_result <- anova(step_model)
print(anova_result)
```
The cross validation tells us that our model is not over fitting since the $R^2$ is only about 35%, but also suggests that we can improve our model because the RMSE is quite high at 54993, so we may need to add more interactions or do some non linear transformations of the data.

The residual plot shows a funnel shape, which suggests hetereoscadascity of the variance and the residuals do not appear to be centered at zero, which indicates that there is bias. The plots indicate that we may want to use the log of the salary instead of salary as our response.

we can now run the step AIC function again using the log(salary) as our response and perform the same model diagnostic tests as above. We also add an interaction term between experience_level and company size.

```{r}
model_df$log_salary <- log(model_df$salary_in_usd)
log_full_model <- lm(log_salary ~ .- salary_in_usd -log_salary, data = model_df)
step_log_model <- stepAIC(log_full_model, direction = "both", trace = FALSE)
summary(step_log_model)

model_with_interaction <- update(step_log_model, 
                                 . ~ . + experience_level:company_size)

# 2. Perform stepwise selection again starting from the updated model
step_log_model_interact <- stepAIC(model_with_interaction, 
                                   direction = "both", 
                                   trace = FALSE)

# 3. Review the summary of the new model
summary(step_log_model_interact)


plot(step_log_model_interact$fitted.values, step_log_model_interact$residuals,
     main = "Residuals vs Fitted",
     xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red")

qqnorm(step_log_model_interact$residuals, main = "Q-Q Plot")
qqline(step_log_model_interact$residuals, col = "red")

bp_test <- bptest(step_log_model_interact)
print(bp_test)  # p-value < 0.05 indicates potential heteroscedasticity

anova_result <- anova(step_log_model_interact)
print(anova_result)
```
Our log transformed model has a much higher $R^2$ indicating that our model explains a lot more of the variability in log(salary) than it does salary. Although our residual plot still heteroscedasity, so we need to consider what other transformations we can make.

```{r}
bc <- boxcox(lm(salary_in_usd ~ -salary_in_usd, data = model_df), plotit = TRUE)
# Choose lambda based on the plot and refit using transformed response:
lambda_opt <- bc$x[which.max(bc$y)]
model_df$trans_salary <- if(lambda_opt == 0) log(model_df$salary_in_usd) else (model_df$salary_in_usd^lambda_opt - 1)/lambda_opt

```
The box cox indicates that the log transformed model is the best power model for our data, so the changes we need to make will be on individual predictors and not the entire data.

## 2.5 Model Descriptions

After comparing multiple candidate models, our final chosen model regresses **log(salary_in_usd)** on several predictors:

\[
\log(\text{salary\_in\_usd}) =
\beta_0 
+ \beta_1 \cdot \text{experience\_level} 
+ \beta_2 \cdot \text{job\_title} 
+ \beta_3 \cdot \text{employee\_residence}
+ \beta_4 \cdot \text{work\_setting}
+ \beta_5 \cdot \text{company\_size}
+ \beta_6 \cdot (\text{experience\_level} \times \text{company\_size})
+ \cdots
\]

### 1. **Log Transformation**
We transformed the salary to \(\log(\text{salary\_usd})\) to address heteroscedasticity and non‐normal residuals discovered in the initial model. Interpreting coefficients on the log‐scale means that each unit increase in a predictor corresponds to a *multiplicative* change in salary, rather than an additive one. For example:
- If \(\beta_1 = 0.10\) for a given predictor \(X\), then a **1‐unit** increase in \(X\) is associated with about a **10.5%** increase in salary \(\bigl(e^{0.10} - 1 \approx 0.105\bigr)\).

### 2. **Baseline Levels and Interpretation**
Many of our predictors are categorical. By default, R sets the first factor level of each predictor as the “baseline,” and the model estimates how other levels differ **relative** to that baseline. Specifically:
- **experience_level**: “Entry‐level” is our baseline. Coefficients for “Mid‐level,” “Senior,” and “Executive” show how salaries differ (in log‐units) from entry‐level roles.
- **company_size**: “S” (small) is our baseline. Coefficients for “M” and “L” measure how medium or large companies differ from small ones in terms of salary.
- **job_title, work_setting, employee_residence**: Similar logic applies—each coefficient is relative to its baseline category.

### 3. **Significant Predictors**
Based on the final model summary (not shown here in detail), we typically see the following patterns:
- **experience_level**: Highly significant, with more senior roles (e.g., “Senior,” “Executive”) associated with higher \(\log(\text{salary})\). This implies strong upward salary trends as experience grows.
- **company_size**: Medium or large companies might pay more, on average, than small companies—but the magnitude and significance can vary by the data.
- **job_title**: Certain roles (e.g., “Data Scientist,” “Machine Learning”) often command higher salaries relative to baseline roles.
- **employee_residence**: Countries or regions with different cost‐of‐living indices can have substantially different salary norms.
- **work_setting**: If “Remote” vs. “In‐Office” or other setups are included, each may show a different average salary level.

### 4. **Interaction: `experience_level : company_size`**
We included an interaction between **experience_level** and **company_size** to test whether the *effect* of experience on salary depends on the size of the company. Interpreting an interaction on the log‐scale:
- The coefficient of `(Senior : Large)` tells us the additional log‐salary *beyond* simply adding the main effects of “Senior” and “Large” separately.  
- If this interaction is positive and significant, it indicates that seniority in larger companies *amplifies* the salary effect compared to small companies.

### 5. **Goodness‐of‐Fit**
- **\(R^2\) on the log scale**: While a higher \(R^2\) indicates better explanatory power, we are primarily interested in whether major assumptions (linearity, normality of residuals) are met.
- **RMSE or MSE** on the **log scale**: Tells us how far predictions deviate from actual \(\log(\text{salary})\). In exponentiated form, it helps gauge the typical percentage error in predictions.

### 6. **Exponentiating Predictions to Get Actual Salary**
Since the model predicts \(\log(\text{salary})\):
\[
\hat{y}_{\text{log}} = \beta_0 \;+\;\beta_1 x_1 \;+\;\dots
\]
To get back to the **predicted salary**, compute:
\[
\hat{\text{salary}} = e^{\hat{y}_{\text{log}}}
\]
Any confidence or prediction intervals on the log scale can similarly be exponentiated to get intervals in dollar terms.






# 3 Testing and Results

```{r test-set-results, echo=FALSE}
# 1. Fit the final model on the "candidate_set"
#    (This assumes you've decided on a final formula from AIC/BIC or stepwise.)
final_model <- step_log_model_interact

# 2. Generate predictions on the test set
test_predictions_log <- predict(final_model, newdata = test_set)

# 3. Evaluate performance
#    (a) MSE on the log scale
test_mse_log <- mean((log(test_set$salary_in_usd) - test_predictions_log)^2)

#    (b) RMSE on the log scale
test_rmse_log <- sqrt(test_mse_log)

#    (c) Convert predictions from log-salary to salary
test_predictions_salary <- exp(test_predictions_log)

#    (d) MSE on the *original* salary scale (optional)
#        We'll compare predicted salary vs. actual salary_in_usd
test_mse_salary <- mean((test_set$salary_in_usd - test_predictions_salary)^2)
test_rmse_salary <- sqrt(test_mse_salary)

# 4. Print out the results
cat("Test Set Results (Log-Scale):\n")
cat(" - MSE (log-scale):", round(test_mse_log, 4), "\n")
cat(" - RMSE (log-scale):", round(test_rmse_log, 4), "\n\n")

cat("Test Set Results (Original Salary Scale):\n")
cat(" - MSE (USD):", round(test_mse_salary, 2), "\n")
cat(" - RMSE (USD):", round(test_rmse_salary, 2), "\n")

```


```{r test-set-plot, echo=FALSE, fig.width=5, fig.height=4}
results_df <- data.frame(
  actual_salary = test_set$salary_in_usd,
  predicted_salary = test_predictions_salary
)

ggplot(results_df, aes(x = actual_salary, y = predicted_salary)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(title = "Test Set: Predicted vs. Actual Salary",
       x = "Actual Salary (USD)",
       y = "Predicted Salary (USD)") +
  theme_minimal()
```


# 4. Conclusion

# Appendix

Data Manipulation Process:

```{r ref.label="data-manipulation", eval=FALSE}
```

Normality check in section 2

```{r ref.label="normality-check-1", eval=FALSE}

```

Checking if interactions are necessary

```{r ref.label="interaction-check-1", eval=FALSE}

```

VIF check

```{r ref.label="vif-check", eval=FALSE}

```

Correlation Matrix:

```{r ref.label="correlation-vif", eval=FALSE}

```